import logging
import warnings
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
import json
import datetime
import os

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv, global_mean_pool

logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')

RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

@dataclass
class ModelConfig:
    node_features: int = 12
    hidden_dim: int = 64
    num_gcn_layers: int = 3
    lstm_hidden: int = 32
    dropout_rate: float = 0.2
    
    learning_rate: float = 0.001
    weight_decay: float = 1e-4
    batch_size: int = 16
    epochs: int = 100
    sequence_length: int = 3
    patience: int = 20
    
    model_ensemble_size: int = 3
    use_cpu: bool = True

@dataclass
class ValidationResults:
    cv_mae: float
    test_mae: float
    baseline_persistence: float
    baseline_historical: float
    r2_score: float
    predictions: Dict[str, float]

class ModelLogger:
    def __init__(self, base_dir: str = "experiments"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)
        self.run_dir = None
        self.log_data = []
        self.config_data = {}
        self.analysis_log = []
        
    def create_run_directory(self, config: 'ModelConfig', run_name: str = None):
        if run_name is None:
            existing_runs = []
            if self.base_dir.exists():
                for item in self.base_dir.iterdir():
                    if item.is_dir() and item.name.startswith('gcn_run_'):
                        try:
                            run_num = int(item.name.split('_')[-1])
                            existing_runs.append(run_num)
                        except ValueError:
                            continue
            
            next_run = max(existing_runs, default=0) + 1
            run_name = f"gcn_run_{next_run}"
            
        self.run_dir = self.base_dir / run_name
        self.run_dir.mkdir(exist_ok=True)
        
        self.config_data = asdict(config)
        self.log_data = []
        self.analysis_log = []
        
        self._save_config()
        print(f"Created experiment directory: {self.run_dir}")
        
        self.log_analysis_step("Experiment Started", f"Created directory: {self.run_dir}")
        
    def log_analysis_step(self, step_name: str, details: str):
        timestamp = datetime.datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] {step_name}: {details}"
        self.analysis_log.append(log_entry)
        
    def log_epoch(self, epoch: int, train_loss: float, val_loss: float = None):
        if val_loss is not None:
            log_line = f"Epoch {epoch:3d}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}"
        else:
            log_line = f"Epoch {epoch:3d}: Train Loss = {train_loss:.6f}"
        self.log_data.append(log_line)
        
    def save_training_log(self):
        if self.run_dir:
            log_path = self.run_dir / "training_log.txt"
            with open(log_path, 'w') as f:
                f.write("Training Log\n")
                f.write("=" * 50 + "\n\n")
                for line in self.log_data:
                    f.write(line + "\n")
                    
    def save_final_metrics(self, results: ValidationResults):
        if self.run_dir:
            metrics = {
                "experiment_info": {
                    "run_name": self.run_dir.name,
                    "timestamp": datetime.datetime.now().isoformat(),
                    "data_source": "FAO Wheat Trade Data"
                },
                "performance_metrics": {
                    "test_mae": float(results.test_mae),
                    "r2_score": float(results.r2_score),
                    "num_predictions": len(results.predictions)
                },
                "baseline_comparison": {
                    "baseline_persistence": float(results.baseline_persistence),
                    "baseline_historical": float(results.baseline_historical),
                    "improvement_vs_persistence_pct": float((results.baseline_persistence - results.test_mae) / results.baseline_persistence * 100) if results.baseline_persistence != float('inf') else 0.0,
                    "improvement_vs_historical_pct": float((results.baseline_historical - results.test_mae) / results.baseline_historical * 100) if results.baseline_historical != float('inf') else 0.0
                },
                "model_files": {
                    "best_model": "best_model.pt",
                    "training_log": "training_log.txt",
                    "analysis_log": "analysis_log.txt",
                    "config": "config.txt"
                }
            }
            
            metrics_path = self.run_dir / "final_metrics.json"
            with open(metrics_path, 'w') as f:
                json.dump(metrics, f, indent=2)
                
    def _save_config(self):
        if self.run_dir:
            config_path = self.run_dir / "config.txt"
            with open(config_path, 'w') as f:
                f.write("Wheat Vulnerability GNN - Experiment Configuration\n")
                f.write("=" * 55 + "\n\n")
                f.write("Model Architecture:\n")
                f.write(f"  node_features: {self.config_data['node_features']}\n")
                f.write(f"  hidden_dim: {self.config_data['hidden_dim']}\n")
                f.write(f"  num_gcn_layers: {self.config_data['num_gcn_layers']}\n")
                f.write(f"  lstm_hidden: {self.config_data['lstm_hidden']}\n")
                f.write(f"  dropout_rate: {self.config_data['dropout_rate']}\n\n")
                
                f.write("Training Parameters:\n")
                f.write(f"  learning_rate: {self.config_data['learning_rate']}\n")
                f.write(f"  weight_decay: {self.config_data['weight_decay']}\n")
                f.write(f"  batch_size: {self.config_data['batch_size']}\n")
                f.write(f"  epochs: {self.config_data['epochs']}\n")
                f.write(f"  sequence_length: {self.config_data['sequence_length']}\n")
                f.write(f"  patience: {self.config_data['patience']}\n\n")
                
                f.write("Other Settings:\n")
                f.write(f"  model_ensemble_size: {self.config_data['model_ensemble_size']}\n")
                f.write(f"  use_cpu: {self.config_data['use_cpu']}\n")
                f.write(f"\nExperiment Run: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    
    def get_model_path(self) -> str:
        if self.run_dir:
            return str(self.run_dir / "best_model.pt")
        return "best_model.pt"
    
    def save_final_model(self, model, config: 'ModelConfig', final_metrics: dict):
   
        if self.run_dir:
            model_path = self.run_dir / "best_model.pt"
            torch.save({
                'model_state_dict': model.state_dict(),
                'config': asdict(config),
                'final_metrics': final_metrics,
                'model_architecture': str(model),
                'timestamp': datetime.datetime.now().isoformat()
            }, model_path)
            print(f"Model saved: {model_path}")
            
    def save_analysis_log(self):
        
        if self.run_dir:
            log_path = self.run_dir / "analysis_log.txt"
            with open(log_path, 'w') as f:
                f.write("Wheat Vulnerability GNN - Analysis Pipeline Log\n")
                f.write("=" * 60 + "\n\n")
                f.write(f"Experiment: {self.run_dir.name}\n")
                f.write(f"Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write("Pipeline Steps:\n")
                f.write("-" * 30 + "\n")
                for entry in self.analysis_log:
                    f.write(entry + "\n")
                f.write("\n" + "=" * 60 + "\n")

class DataProcessor:
    
    def __init__(self, data_path: Union[str, Path] = 'data/wheat_trade_data.csv'):
        self.data_path = Path(data_path)
        self.scaler = StandardScaler()
        self._validate_data_path()
    
    def _validate_data_path(self) -> None:
        if not self.data_path.exists():
            raise FileNotFoundError(f"Data file not found: {self.data_path}")
    
    def load_wheat_data(self) -> pd.DataFrame:
        df = pd.read_csv(self.data_path)
        
        required_cols = ['Year', 'Reporter Countries', 'Partner Countries', 'Value', 'Element Code']
        missing_cols = set(required_cols) - set(df.columns)
        if missing_cols:
            raise ValueError(f"Missing required columns: {missing_cols}")
        
        imports = (df[df['Element Code'] == 5610]
                  [['Year', 'Reporter Countries', 'Partner Countries', 'Value']]
                  .copy())
        
        imports.columns = ['year', 'importer', 'exporter', 'value']
        
        imports = (imports
                  .query('year >= 2000 and year <= 2020')
                  .query('value > 0')
                  .dropna()
                  .groupby(['year', 'importer', 'exporter'])['value']
                  .sum()
                  .reset_index())
        
        return imports
    
    def estimate_consumption_baselines(self, data: pd.DataFrame = None) -> Dict[str, float]:
        if data is None:
            data = self.load_wheat_data()
        
        yearly_imports = data.groupby(['year', 'importer'])['value'].sum().reset_index()
        yearly_exports = data.groupby(['year', 'exporter'])['value'].sum().reset_index()
        yearly_exports.columns = ['year', 'country', 'exports']
        yearly_imports.columns = ['year', 'country', 'imports']
        
        trade_balance = pd.merge(yearly_imports, yearly_exports, on=['year', 'country'], how='outer').fillna(0)
        trade_balance['net_imports'] = trade_balance['imports'] - trade_balance['exports']
        
        consumption = (trade_balance
                      .groupby('country')['net_imports']
                      .agg(['median', 'count'])
                      .reset_index())
        
        consumption = consumption.query('count >= 5 and median > 0')
        
        baseline_dict = dict(zip(consumption['country'], consumption['median']))
        
        return baseline_dict

    def calculate_vulnerability_targets(self, data: pd.DataFrame = None, 
                                      consumption_baselines: Dict[str, float] = None) -> Dict[int, Dict[str, float]]:
        if data is None:
            data = self.load_wheat_data()
        if consumption_baselines is None:
            consumption_baselines = self.estimate_consumption_baselines(data)
        
        targets = {}
        
        # Get global statistics for normalization
        all_countries = list(consumption_baselines.keys())
        global_imports_by_year = data.groupby('year')['value'].sum()
        
        for year in sorted(data['year'].unique()):
            year_data = data[data['year'] == year]
            year_imports = year_data.groupby('importer')['value'].sum()
            year_targets = {}
            
            # Calculate various vulnerability components
            vulnerability_components = {}
            
            for country in all_countries:
                actual_imports = year_imports.get(country, 0)
                baseline = consumption_baselines[country]
                
                # 1. Import Adequacy (0-1, higher = more vulnerable)
                import_adequacy = max(0, min(1, 1 - (actual_imports / baseline)))
                
                # 2. Supply Concentration Risk (Herfindahl index)
                country_trade = year_data[year_data['importer'] == country]
                if not country_trade.empty:
                    supplier_shares = country_trade.groupby('exporter')['value'].sum()
                    total_imports = supplier_shares.sum()
                    if total_imports > 0:
                        concentration = sum((share / total_imports) ** 2 for share in supplier_shares)
                    else:
                        concentration = 1.0  # Maximum concentration if no imports
                else:
                    concentration = 1.0  # No suppliers = maximum risk
                
                # 3. Supplier Diversity (fewer suppliers = higher risk)
                num_suppliers = len(country_trade['exporter'].unique()) if not country_trade.empty else 0
                supplier_diversity_risk = max(0, min(1, 1 - (num_suppliers / 10)))  # Normalize to 10 suppliers
                
                # 4. Geopolitical Risk (dependency on unstable regions)
                geopolitical_suppliers = ['Russian Federation', 'Ukraine', 'Belarus', 'Kazakhstan']
                geo_exposure = 0
                if not country_trade.empty and total_imports > 0:
                    for geo_supplier in geopolitical_suppliers:
                        geo_imports = country_trade[country_trade['exporter'] == geo_supplier]['value'].sum()
                        geo_exposure += geo_imports / total_imports
                geo_risk = min(1, geo_exposure)
                
                # 5. Trade Volatility (historical import stability)
                historical_imports = []
                for hist_year in range(max(2000, year-5), year):
                    if hist_year in data['year'].unique():
                        hist_data = data[data['year'] == hist_year]
                        hist_import = hist_data[hist_data['importer'] == country]['value'].sum()
                        historical_imports.append(hist_import)
                
                if len(historical_imports) > 1:
                    volatility = np.std(historical_imports) / (np.mean(historical_imports) + 1)
                    volatility_risk = min(1, volatility / 2)  # Normalize
                else:
                    volatility_risk = 0.5  # Default for new countries
                
                # 6. Economic Size Factor (smaller economies more vulnerable)
                # Use imports as proxy for economic involvement in wheat trade
                max_imports = year_imports.max() if not year_imports.empty else 1
                economic_size_factor = 1 - (actual_imports / max_imports) if max_imports > 0 else 1
                
                # Combine components with weights
                vulnerability = (
                    import_adequacy * 0.25 +           # Import shortfall
                    concentration * 0.20 +             # Supply concentration
                    supplier_diversity_risk * 0.15 +   # Lack of diversity
                    geo_risk * 0.15 +                  # Geopolitical exposure
                    volatility_risk * 0.15 +           # Trade instability
                    economic_size_factor * 0.10        # Economic vulnerability
                )
                
                # Ensure vulnerability is between 0 and 1
                vulnerability = max(0, min(1, vulnerability))
                
                vulnerability_components[country] = {
                    'import_adequacy': import_adequacy,
                    'concentration': concentration,
                    'supplier_diversity_risk': supplier_diversity_risk,
                    'geo_risk': geo_risk,
                    'volatility_risk': volatility_risk,
                    'economic_size_factor': economic_size_factor,
                    'final_vulnerability': vulnerability
                }
                
                year_targets[country] = vulnerability
            
            targets[year] = year_targets
            
            # Log some statistics for this year
            vulnerabilities = list(year_targets.values())
            if vulnerabilities:
                print(f"   {year}: Vulnerability range {min(vulnerabilities):.3f}-{max(vulnerabilities):.3f}, "
                      f"mean {np.mean(vulnerabilities):.3f}, std {np.std(vulnerabilities):.3f}")
        
        return targets

class GraphBuilder:
    
    def __init__(self, config: ModelConfig):
        self.config = config
    
    def build_temporal_graph(self, data: pd.DataFrame, year: int, countries: List[str]) -> Data:
        year_data = data[data['year'] == year]
        country_to_idx = {country: idx for idx, country in enumerate(countries)}
        n_countries = len(countries)
        
        if year_data.empty:
            return Data(
                x=torch.zeros(n_countries, self.config.node_features),
                edge_index=torch.empty(2, 0, dtype=torch.long),
                edge_attr=torch.empty(0, 1)
            )
        
        edge_sources, edge_targets, edge_weights = [], [], []
        for _, row in year_data.iterrows():
            src_idx = country_to_idx.get(row['exporter'])
            tgt_idx = country_to_idx.get(row['importer'])
            
            if src_idx is not None and tgt_idx is not None:
                edge_sources.append(src_idx)
                edge_targets.append(tgt_idx)
                edge_weights.append(np.log1p(row['value']))
        
        node_features = self._compute_node_features(data, year, countries, country_to_idx)
        
        return Data(
            x=torch.tensor(node_features, dtype=torch.float32),
            edge_index=torch.tensor([edge_sources, edge_targets], dtype=torch.long) if edge_sources else torch.empty(2, 0, dtype=torch.long),
            edge_attr=torch.tensor(edge_weights, dtype=torch.float32).unsqueeze(1) if edge_weights else torch.empty(0, 1)
        )
    
    def _compute_node_features(self, data: pd.DataFrame, year: int, countries: List[str], 
                             country_to_idx: Dict[str, int]) -> np.ndarray:
        features = np.zeros((len(countries), self.config.node_features))
        
        for idx, country in enumerate(countries):
            feature_years = [y for y in range(year-3, year+1) if y >= data['year'].min()]
            
            import_history = []
            export_history = []
            partner_counts = []
            
            for fy in feature_years:
                year_data = data[data['year'] == fy]
                imports = year_data[year_data['importer'] == country]['value'].sum()
                exports = year_data[year_data['exporter'] == country]['value'].sum()
                
                import_partners = len(year_data[year_data['importer'] == country]['exporter'].unique())
                export_partners = len(year_data[year_data['exporter'] == country]['importer'].unique())
                
                import_history.append(np.log1p(imports))
                export_history.append(np.log1p(exports))
                partner_counts.append(import_partners + export_partners)
            
            current_imports = import_history[-1] if import_history else 0
            current_exports = export_history[-1] if export_history else 0
            current_partners = partner_counts[-1] if partner_counts else 0
            
            import_trend = np.polyfit(range(len(import_history)), import_history, 1)[0] if len(import_history) > 1 else 0
            export_trend = np.polyfit(range(len(export_history)), export_history, 1)[0] if len(export_history) > 1 else 0
            import_volatility = np.std(import_history) if len(import_history) > 1 else 0
            export_volatility = np.std(export_history) if len(export_history) > 1 else 0
            
            trade_balance = current_exports - current_imports
            trade_openness = current_imports + current_exports
            
            import_concentration = self._calculate_concentration(data, year, country, 'importer')
            export_concentration = self._calculate_concentration(data, year, country, 'exporter')
            centrality_score = self._calculate_centrality(data, year, country)
            
            features[idx] = [
                current_imports,
                current_exports,
                current_partners,
                trade_balance,
                import_trend,
                export_trend,
                import_volatility,
                export_volatility,
                trade_openness,
                import_concentration,
                export_concentration,
                centrality_score
            ]
        
        return features
    
    def _calculate_concentration(self, data: pd.DataFrame, year: int, country: str, role: str) -> float:
        year_data = data[data['year'] == year]
        if role == 'importer':
            country_data = year_data[year_data['importer'] == country]
        else:
            country_data = year_data[year_data['exporter'] == country]
        
        total_trade = country_data['value'].sum()
        if total_trade == 0:
            return 0
        
        shares = (country_data['value'] / total_trade) ** 2
        return shares.sum()
    
    def _calculate_centrality(self, data: pd.DataFrame, year: int, country: str) -> float:
        year_data = data[data['year'] == year]
        countries = list(set(year_data['importer'].tolist() + year_data['exporter'].tolist()))
        
        if country not in countries:
            return 0
        
        in_degree = len(year_data[year_data['importer'] == country]['exporter'].unique())
        out_degree = len(year_data[year_data['exporter'] == country]['importer'].unique())
        
        return (in_degree + out_degree) / max(len(countries) - 1, 1)

class VulnerabilityGNN(nn.Module):
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        
        self.gcn_layers = nn.ModuleList([
            GCNConv(config.node_features if i == 0 else config.hidden_dim, config.hidden_dim)
            for i in range(config.num_gcn_layers)
        ])
        
        self.lstm = nn.LSTM(
            config.hidden_dim, 
            config.lstm_hidden, 
            batch_first=True, 
            dropout=config.dropout_rate
        )
        
        self.predictor = nn.Sequential(
            nn.Linear(config.lstm_hidden, config.hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(config.dropout_rate),
            nn.Linear(config.hidden_dim // 2, 1),
            nn.Sigmoid()
        )
        
        self.dropout = nn.Dropout(config.dropout_rate)
    
    def forward(self, graph_sequence: List[Data]) -> torch.Tensor:
        # Process each graph in the sequence
        batch_size = len(graph_sequence)
        n_countries = graph_sequence[0].x.size(0)
        
        # Store embeddings for each time step
        temporal_embeddings = []
        
        for graph in graph_sequence:
            x = graph.x
            
            # Apply GCN layers
            for gcn in self.gcn_layers:
                x = gcn(x, graph.edge_index)
                x = torch.relu(x)
                x = self.dropout(x)
            
            # x shape: [n_countries, hidden_dim]
            temporal_embeddings.append(x)
        
        # Stack temporal embeddings: [n_countries, seq_len, hidden_dim]
        if len(temporal_embeddings) > 1:
            temporal_sequence = torch.stack(temporal_embeddings, dim=1)
            
            # Apply LSTM to each country's temporal sequence
            lstm_out, _ = self.lstm(temporal_sequence)
            # lstm_out shape: [n_countries, seq_len, lstm_hidden]
            
            # Use the last time step for prediction
            final_embedding = lstm_out[:, -1, :]  # [n_countries, lstm_hidden]
        else:
            final_embedding = temporal_embeddings[0]
        
        # Generate vulnerability predictions for all countries
        vulnerability_pred = self.predictor(final_embedding)
        # Output shape: [n_countries, 1]
        
        return vulnerability_pred

class ModelTrainer:
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.device = torch.device('cpu')
        self.model_logger = ModelLogger(base_dir=r"C:\Users\Windows 11\Desktop\Journal\github_repo\experiments")
    
    def train_model(self, train_sequences: List[Tuple], val_sequences: List[Tuple] = None, experiment_name: str = None) -> VulnerabilityGNN:
        self.model_logger.create_run_directory(self.config, experiment_name)
        
        model = VulnerabilityGNN(self.config).to(self.device)
        
        optimizer = torch.optim.AdamW(
            model.parameters(), 
            lr=self.config.learning_rate, 
            weight_decay=self.config.weight_decay
        )
        
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', patience=self.config.patience // 3, factor=0.5
        )
        
        criterion = nn.MSELoss()
        best_val_loss = float('inf')
        patience_counter = 0
        best_model_path = self.model_logger.get_model_path()
        
        for epoch in range(self.config.epochs):
            model.train()
            train_loss = 0.0
            
            if epoch % 10 == 0 or epoch == self.config.epochs - 1:
                progress = (epoch + 1) / self.config.epochs * 100
                print(f"Training Progress: {progress:.1f}% (Epoch {epoch + 1}/{self.config.epochs})")
            
            for seq, target in train_sequences:
                optimizer.zero_grad()
                predictions = model(seq)
                loss = criterion(predictions, target.to(self.device))
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
            
            train_loss /= len(train_sequences)
            
            val_loss = 0.0
            if val_sequences:
                model.eval()
                with torch.no_grad():
                    for seq, target in val_sequences:
                        predictions = model(seq)
                        loss = criterion(predictions, target.to(self.device))
                        val_loss += loss.item()
                
                val_loss /= len(val_sequences)
                scheduler.step(val_loss)
                
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                    torch.save({
                        'model_state_dict': model.state_dict(),
                        'config': asdict(self.config),
                        'epoch': epoch,
                        'train_loss': train_loss,
                        'val_loss': val_loss
                    }, best_model_path)
                    if epoch % 10 == 0 or epoch == self.config.epochs - 1:
                        print(f"  ‚Üí New best model saved! Val Loss: {val_loss:.6f}")
                else:
                    patience_counter += 1
                
                if patience_counter >= self.config.patience:
                    print(f"  ‚Üí Early stopping at epoch {epoch + 1} (patience reached)")
                    break
                    
                if epoch % 10 == 0 or epoch == self.config.epochs - 1:
                    print(f"  ‚Üí Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")
            else:
                if train_loss < best_val_loss:
                    best_val_loss = train_loss
                    torch.save({
                        'model_state_dict': model.state_dict(),
                        'config': asdict(self.config),
                        'epoch': epoch,
                        'train_loss': train_loss,
                        'val_loss': 0.0
                    }, best_model_path)
                    if epoch % 10 == 0 or epoch == self.config.epochs - 1:
                        print(f"  ‚Üí New best model saved! Train Loss: {train_loss:.6f}")
                
                if epoch % 10 == 0 or epoch == self.config.epochs - 1:
                    print(f"  ‚Üí Train Loss: {train_loss:.6f}")
            
            self.model_logger.log_epoch(epoch, train_loss, val_loss)
        
        if not os.path.exists(best_model_path):
            torch.save({
                'model_state_dict': model.state_dict(),
                'config': asdict(self.config),
                'epoch': self.config.epochs - 1,
                'train_loss': train_loss,
                'val_loss': val_loss
            }, best_model_path)
        
        if os.path.exists(best_model_path):
            checkpoint = torch.load(best_model_path)
            model.load_state_dict(checkpoint['model_state_dict'])
        
        return model
    
    def evaluate_model(self, model: VulnerabilityGNN, test_sequences: List[Tuple]) -> Tuple[float, np.ndarray, np.ndarray]:
        model.eval()
        predictions, actuals = [], []
        
        with torch.no_grad():
            for seq, target in test_sequences:
                pred = model(seq)
                predictions.append(pred.cpu().numpy())
                actuals.append(target.numpy())
        
        if predictions:
            pred_array = np.concatenate(predictions, axis=0).flatten()
            actual_array = np.concatenate(actuals, axis=0).flatten()
            
            min_len = min(len(pred_array), len(actual_array))
            pred_array = pred_array[:min_len]
            actual_array = actual_array[:min_len]
            
            mae = mean_absolute_error(actual_array, pred_array)
            return mae, pred_array, actual_array
        else:
            return float('inf'), np.array([]), np.array([])

def prepare_sequences(data: pd.DataFrame, targets: Dict[int, Dict[str, float]], 
                     countries: List[str], target_years: List[int], seq_len: int,
                     graph_builder: GraphBuilder, all_years: List[int] = None) -> Tuple[List, List]:

    sequences, sequence_targets = [], []
    
    if all_years is None:
        all_years = sorted(data['year'].unique())
    
    for target_year in target_years:
        if target_year not in targets:
            continue
            
        # Find the seq_len years before target_year from all_years
        input_years = [y for y in all_years if y < target_year][-seq_len:]
        
        if len(input_years) < seq_len:
            print(f"   Warning: Not enough input years for target {target_year}. Need {seq_len}, got {len(input_years)}")
            continue
        
        graphs = []
        for year in input_years:
            graph = graph_builder.build_temporal_graph(data, year, countries)
            graphs.append(graph)
        
        # Create target tensor for ALL countries (this was the key issue)
        target_dict = targets[target_year]
        target_tensor = torch.zeros(len(countries), 1)
        
        countries_with_targets = 0
        for j, country in enumerate(countries):
            if country in target_dict:
                target_tensor[j, 0] = target_dict[country]
                countries_with_targets += 1
        
        print(f"   Target {target_year}: {countries_with_targets}/{len(countries)} countries have vulnerability scores")
        
        sequences.append(graphs)
        sequence_targets.append(target_tensor)
    
    return sequences, sequence_targets

def baseline_comparison(data: pd.DataFrame, targets: Dict[int, Dict[str, float]], 
                       countries: List[str]) -> Dict[str, float]:
    test_years = [2020]  # Updated to match new test approach
    
    if len(test_years) < 1:
        return {'persistence': float('inf'), 'historical_avg': float('inf')}
    
    all_vulnerabilities = [v for year_targets in targets.values() for v in year_targets.values()]
    hist_avg = np.mean(all_vulnerabilities)
    
    persistence_errors, hist_avg_errors = [], []
    
    # For persistence baseline, use 2019 to predict 2020
    if 2020 in targets and 2019 in targets:
        current = np.array([targets[2020].get(c, 0) for c in countries])
        previous = np.array([targets[2019].get(c, 0) for c in countries])
        hist_avg_pred = np.full_like(current, hist_avg)
        
        persistence_errors.append(mean_absolute_error(current, previous))
        hist_avg_errors.append(mean_absolute_error(current, hist_avg_pred))
    
    return {
        'persistence': np.mean(persistence_errors) if persistence_errors else float('inf'),
        'historical_avg': np.mean(hist_avg_errors) if hist_avg_errors else float('inf')
    }

def run_analysis(experiment_name: str = None) -> Optional[ValidationResults]:
    config = ModelConfig()
    data_processor = DataProcessor()
    graph_builder = GraphBuilder(config)
    trainer = ModelTrainer(config)
    
    try:
        print("üìä Loading and processing wheat trade data...")
        data = data_processor.load_wheat_data()
        print(f"   ‚Üí Loaded {len(data)} trade records")
        trainer.model_logger.log_analysis_step("Data Loading", f"Loaded {len(data)} trade records from wheat trade data")
        
        print("üîç Calculating consumption baselines...")
        consumption_baselines = data_processor.estimate_consumption_baselines(data)
        print(f"   ‚Üí Estimated baselines for {len(consumption_baselines)} countries")
        trainer.model_logger.log_analysis_step("Baseline Calculation", f"Estimated baselines for {len(consumption_baselines)} countries")
        
        print("üéØ Computing vulnerability targets...")
        targets = data_processor.calculate_vulnerability_targets(data, consumption_baselines)
        countries = list(consumption_baselines.keys())
        print(f"   ‚Üí Computed targets for {len(targets)} years")
        trainer.model_logger.log_analysis_step("Target Computation", f"Computed vulnerability targets for {len(targets)} years across {len(countries)} countries")
        
        print("üìà Computing baseline comparisons...")
        baseline_results = baseline_comparison(data, targets, countries)
        trainer.model_logger.log_analysis_step("Baseline Comparison", f"Persistence MAE: {baseline_results['persistence']:.4f}, Historical MAE: {baseline_results['historical_avg']:.4f}")
        
        all_years = sorted(data['year'].unique())
        # Use full dataset for training with time-series cross-validation
        # Train on 2000-2018, validate on 2019, test on 2020 (walk-forward approach)
        train_years = [y for y in all_years if y <= 2018]  # 2000-2018 (19 years)
        val_years = [2019]  # Single year for validation
        test_years = [2020]  # Single year for final test
        
        print(f"üîÑ Preparing sequences with walk-forward validation...")
        print(f"   ‚Üí Training years: {train_years[0]}-{train_years[-1]} ({len(train_years)} years)")
        print(f"   ‚Üí Validation year: {val_years[0]} (1 year)")
        print(f"   ‚Üí Test year: {test_years[0]} (1 year)")
        trainer.model_logger.log_analysis_step("Data Split", f"Training: {train_years[0]}-{train_years[-1]} ({len(train_years)} years), Validation: {val_years[0]} (1 year), Test: {test_years[0]} (1 year)")
        
        train_seq, train_tgt = prepare_sequences(data, targets, countries, train_years, config.sequence_length, graph_builder, all_years)
        val_seq, val_tgt = prepare_sequences(data, targets, countries, val_years, config.sequence_length, graph_builder, all_years)
        test_seq, test_tgt = prepare_sequences(data, targets, countries, test_years, config.sequence_length, graph_builder, all_years)
        
        if not train_seq:
            print("‚ùå No training sequences generated!")
            trainer.model_logger.log_analysis_step("ERROR", "No training sequences generated - analysis failed")
            return None
        
        print(f"   ‚Üí Generated {len(train_seq)} training sequences")
        print(f"   ‚Üí Generated {len(val_seq) if val_seq else 0} validation sequences")
        print(f"   ‚Üí Generated {len(test_seq)} test sequences")
        trainer.model_logger.log_analysis_step("Sequence Generation", f"Training: {len(train_seq)}, Validation: {len(val_seq) if val_seq else 0}, Test: {len(test_seq)} sequences")
        
        train_data = list(zip(train_seq, train_tgt))
        val_data = list(zip(val_seq, val_tgt)) if val_seq else None
        test_data = list(zip(test_seq, test_tgt)) if test_seq else []
        
        print(f"\nüöÄ Starting model training...")
        print(f"   ‚Üí Model: {config.num_gcn_layers}-layer GCN + LSTM")
        print(f"   ‚Üí Learning rate: {config.learning_rate}")
        print(f"   ‚Üí Max epochs: {config.epochs}")
        print("-" * 50)
        trainer.model_logger.log_analysis_step("Training Start", f"Model: {config.num_gcn_layers}-layer GCN+LSTM, LR: {config.learning_rate}, Max epochs: {config.epochs}")
        
        model = trainer.train_model(train_data, val_data, experiment_name)
        
        print("\nüß™ Evaluating model on test data...")
        if test_data:
            test_mae, predictions, actuals = trainer.evaluate_model(model, test_data)
            
            ss_res = np.sum((actuals - predictions) ** 2)
            ss_tot = np.sum((actuals - np.mean(actuals)) ** 2)
            r2_score = 1 - (ss_res / (ss_tot + 1e-8))
            
            print(f"   ‚Üí Test predictions generated: {len(predictions)}")
            trainer.model_logger.log_analysis_step("Model Evaluation", f"Test MAE: {test_mae:.4f}, R¬≤: {r2_score:.4f}, Predictions: {len(predictions)}")
        else:
            test_mae, r2_score = float('inf'), 0.0
            predictions = np.array([])
            print("   ‚Üí No test data available")
            trainer.model_logger.log_analysis_step("Model Evaluation", "No test data available")
        
        country_predictions = {}
        if len(predictions) > 0:
            country_predictions = {countries[i % len(countries)]: float(predictions[i]) 
                                 for i in range(min(len(countries), len(predictions)))}
        
        results = ValidationResults(
            cv_mae=0.0,
            test_mae=test_mae,
            baseline_persistence=baseline_results['persistence'],
            baseline_historical=baseline_results['historical_avg'],
            r2_score=r2_score,
            predictions=country_predictions
        )
        
        print("\nüíæ Saving results...")
        trainer.model_logger.save_training_log()
        trainer.model_logger.save_final_metrics(results)
        trainer.model_logger.save_final_model(model, config, {
            'test_mae': test_mae,
            'r2_score': r2_score,
            'baseline_persistence': baseline_results['persistence'],
            'baseline_historical': baseline_results['historical_avg']
        })
        trainer.model_logger.save_analysis_log()
        trainer.model_logger.log_analysis_step("Results Saved", f"All files saved to {trainer.model_logger.run_dir}")
        print("   ‚Üí All files saved successfully!")
        
        return results
        
    except Exception as e:
        raise

def analyze_wheat_vulnerability(model_path: str = None, data_path: str = 'data/wheat_trade_data.csv', 
                              analysis_year: int = 2021, experiment_folder: str = None) -> Dict[str, Dict]:
    analysis_logger = ModelLogger(base_dir="experiments")
    
    if model_path is None:
        experiments_dir = Path("experiments")
        if experiments_dir.exists():
            experiment_dirs = [d for d in experiments_dir.iterdir() if d.is_dir() and d.name.startswith('gcn_run_')]
            if experiment_dirs:
                if experiment_folder:
                    target_experiment = experiments_dir / experiment_folder
                    if target_experiment.exists():
                        latest_experiment = target_experiment
                        analysis_logger.log_analysis_step("Vulnerability Analysis Started", 
                            f"Using specified experiment {experiment_folder} for year {analysis_year}")
                    else:
                        print(f"‚ùå Specified experiment folder '{experiment_folder}' not found!")
                        return {}
                else:
                    latest_experiment = max(experiment_dirs, key=lambda x: int(x.name.split('_')[-1]))
                    analysis_logger.log_analysis_step("Vulnerability Analysis Started", 
                        f"Using latest experiment {latest_experiment.name} for year {analysis_year}")
                
                model_path = latest_experiment / "best_model.pt"
                analysis_logger.run_dir = latest_experiment
            else:
                return {}
        else:
            return {}
    
    try:
        checkpoint = torch.load(model_path, map_location='cpu')
        config = ModelConfig(**checkpoint['config'])
        analysis_logger.log_analysis_step("Model Loading", f"Successfully loaded model with config: {config.num_gcn_layers} GCN layers, {config.node_features} features")
    except Exception as e:
        analysis_logger.log_analysis_step("ERROR", f"Failed to load model: {e}")
        return {}
    
    data_processor = DataProcessor(data_path)
    graph_builder = GraphBuilder(config)
    
    data = data_processor.load_wheat_data()
    consumption_baselines = data_processor.estimate_consumption_baselines(data)
    targets = data_processor.calculate_vulnerability_targets(data, consumption_baselines)
    
    countries = list(consumption_baselines.keys())
    analysis_logger.log_analysis_step("Data Processing", f"Analyzing {len(countries)} countries with {len(data)} trade records")
    
    model = VulnerabilityGNN(config)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    vulnerability_trends = {}
    
    # For future predictions (2026), use the most recent available data
    max_available_year = data['year'].max()
    
    if analysis_year > max_available_year:
        # Future prediction mode - use most recent years as basis
        print(f"üîÆ Predicting future vulnerability for {analysis_year} using data up to {max_available_year}")
        analysis_logger.log_analysis_step("Future Prediction Mode", 
            f"Predicting {analysis_year} vulnerability using latest available data ({max_available_year})")
        
        # Use the most recent sequence_length years for prediction (2018-2020 for 2021 prediction)
        recent_years = list(range(max_available_year - config.sequence_length + 1, max_available_year + 1))
        analysis_years = [analysis_year]  # Only predict for target year
    else:
        # Historical analysis mode
        analysis_years = [analysis_year - 2, analysis_year - 1, analysis_year] if analysis_year >= 2002 else [analysis_year]
    
    for year in analysis_years:
        if analysis_year > max_available_year:
            # For future prediction, use the most recent years
            years_needed = recent_years
        else:
            # For historical analysis, use the original logic
            years_needed = list(range(year - config.sequence_length + 1, year + 1))
            
        available_years = [y for y in years_needed if y in data['year'].unique()]
        
        if len(available_years) < config.sequence_length:
            # Pad with the most recent available year
            latest_year = max(available_years) if available_years else max_available_year
            available_years = available_years + [latest_year] * (config.sequence_length - len(available_years))
        
        graphs = []
        for seq_year in available_years[-config.sequence_length:]:
            graph = graph_builder.build_temporal_graph(data, seq_year, countries)
            graphs.append(graph)
        
        with torch.no_grad():
            vulnerability_scores = model(graphs).squeeze().cpu().numpy()
            vulnerability_trends[year] = vulnerability_scores
    
    analysis_logger.log_analysis_step("Prediction Generation", f"Generated vulnerability predictions for {len(vulnerability_trends)} years")
    
    vulnerability_data = []
    
    for i, country in enumerate(countries):
        country_vulnerabilities = {}
        for year, scores in vulnerability_trends.items():
            if isinstance(scores, np.ndarray):
                if scores.ndim == 0:
                    country_vulnerabilities[year] = float(scores)
                elif len(scores) > i:
                    country_vulnerabilities[year] = float(scores[i])
                else:
                    country_vulnerabilities[year] = 0.0
            else:
                country_vulnerabilities[year] = 0.0
        
        current_vulnerability = country_vulnerabilities.get(analysis_year, 0.0)
        
        if len(country_vulnerabilities) >= 2:
            vuln_values = list(country_vulnerabilities.values())
            vulnerability_trend = np.polyfit(range(len(vuln_values)), vuln_values, 1)[0]
            vulnerability_volatility = np.std(vuln_values)
        else:
            vulnerability_trend = 0.0
            vulnerability_volatility = 0.0
        
        # For trade statistics, use most recent available data if predicting future
        if analysis_year > max_available_year:
            # Use most recent year for trade data
            trade_analysis_year = max_available_year
            analysis_logger.log_analysis_step("Trade Data Source", 
                f"Using {trade_analysis_year} trade data for {analysis_year} prediction")
        else:
            trade_analysis_year = analysis_year
            
        year_data = data[data['year'] == trade_analysis_year]
        country_imports = year_data[year_data['importer'] == country]['value'].sum()
        country_exports = year_data[year_data['exporter'] == country]['value'].sum()
        
        import_sources = year_data[year_data['importer'] == country].groupby('exporter')['value'].sum()
        top_import_source = import_sources.idxmax() if not import_sources.empty else "None"
        import_concentration = (import_sources.max() / import_sources.sum()) if import_sources.sum() > 0 else 0
        
        import_partners = len(import_sources)
        export_destinations = year_data[year_data['exporter'] == country].groupby('importer')['value'].sum()
        export_partners = len(export_destinations)
        
        dependency_ratio = country_imports / max(country_imports + country_exports, 1)
        supply_chain_risk = import_concentration * dependency_ratio
        
        major_suppliers = import_sources.nlargest(3).index.tolist() if not import_sources.empty else []
        geopolitical_risk = sum(import_sources[supplier] / import_sources.sum() 
                               for supplier in major_suppliers if supplier in ['Russian Federation', 'Ukraine', 'China'])
        
        baseline_consumption = consumption_baselines.get(country, 1)
        import_adequacy = country_imports / baseline_consumption if baseline_consumption > 0 else 0
        food_security_index = min(1.0, import_adequacy) * (1 - supply_chain_risk)
        
        vulnerability_data.append({
            'country': country,
            'predicted_vulnerability': current_vulnerability,
            'vulnerability_trend': vulnerability_trend,
            'vulnerability_volatility': vulnerability_volatility,
            'multi_year_scores': country_vulnerabilities,
            'actual_vulnerability': targets.get(trade_analysis_year, {}).get(country, 0) if analysis_year <= max_available_year else None,
            'prediction_year': analysis_year,
            'base_data_year': trade_analysis_year,
            'total_imports': country_imports,
            'total_exports': country_exports,
            'net_imports': country_imports - country_exports,
            'dependency_ratio': dependency_ratio,
            'top_import_source': top_import_source,
            'import_concentration': import_concentration,
            'supply_chain_risk': supply_chain_risk,
            'geopolitical_risk': geopolitical_risk,
            'food_security_index': food_security_index,
            'import_partners': import_partners,
            'export_partners': export_partners,
            'major_suppliers': major_suppliers[:3]
        })
    
    for item in vulnerability_data:
        item['comprehensive_risk'] = (item['predicted_vulnerability'] * 0.4 + 
                                    item['supply_chain_risk'] * 0.3 + 
                                    item['geopolitical_risk'] * 0.2 + 
                                    (1 - item['food_security_index']) * 0.1)
    
    vulnerability_data.sort(key=lambda x: x['comprehensive_risk'], reverse=True)
    
    critical_risk = [c for c in vulnerability_data if c['comprehensive_risk'] > 0.8]
    high_risk = [c for c in vulnerability_data if 0.6 <= c['comprehensive_risk'] <= 0.8]
    medium_risk = [c for c in vulnerability_data if 0.4 <= c['comprehensive_risk'] < 0.6]
    low_risk = [c for c in vulnerability_data if c['comprehensive_risk'] < 0.4]
    
    major_importers = [c for c in vulnerability_data if c['net_imports'] > 1000000]
    high_concentration = [c for c in vulnerability_data if c['import_concentration'] > 0.7]
    geopolitical_vulnerable = [c for c in vulnerability_data if c['geopolitical_risk'] > 0.5]
    limited_partners = [c for c in vulnerability_data if c['import_partners'] < 3 and c['net_imports'] > 0]
    deteriorating_countries = [c for c in vulnerability_data if c['vulnerability_trend'] > 0.1]
    
    analysis_summary = {
        'total_countries_analyzed': len(countries),
        'analysis_year': analysis_year,
        'base_data_year': max_available_year,
        'prediction_type': 'future_prediction' if analysis_year > max_available_year else 'historical_analysis',
        'critical_vulnerabilities': len(critical_risk),
        'major_wheat_importers': len(major_importers),
        'high_supply_concentration': len(high_concentration),
        'geopolitical_risks': len(geopolitical_vulnerable),
        'limited_supplier_diversity': len(limited_partners),
        'deteriorating_food_security': len(deteriorating_countries),
        'average_vulnerability': np.mean([c['predicted_vulnerability'] for c in vulnerability_data]),
        'max_vulnerability': max([c['predicted_vulnerability'] for c in vulnerability_data]),
        'countries_above_baseline': len([c for c in vulnerability_data if c['predicted_vulnerability'] > 0.5])
    }
    
    analysis_logger.log_analysis_step("Risk Categorization", 
        f"Critical: {len(critical_risk)}, High: {len(high_risk)}, Medium: {len(medium_risk)}, Low: {len(low_risk)}")
    
    analysis_logger.log_analysis_step("Vulnerability Insights", 
        f"Major importers: {len(major_importers)}, High concentration: {len(high_concentration)}, "
        f"Geopolitical risk: {len(geopolitical_vulnerable)}, Limited partners: {len(limited_partners)}")
    
    top_10_vulnerable = vulnerability_data[:10]
    analysis_logger.log_analysis_step("Top Vulnerable Countries", 
        f"Most vulnerable: {', '.join([c['country'] for c in top_10_vulnerable[:5]])}")
    
    if analysis_logger.run_dir:
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = analysis_logger.run_dir / f"vulnerability_analysis_{timestamp}.json"
        
        results_data = {
            'vulnerability_rankings': vulnerability_data,
            'risk_categories': {
                'critical_risk': critical_risk,
                'high_risk': high_risk,
                'medium_risk': medium_risk,
                'low_risk': low_risk
            },
            'vulnerability_insights': {
                'major_importers': major_importers,
                'high_concentration': high_concentration,
                'geopolitical_vulnerable': geopolitical_vulnerable,
                'limited_partners': limited_partners,
                'deteriorating_countries': deteriorating_countries
            },
            'analysis_summary': analysis_summary,
            'model_info': {
                'model_path': str(model_path),
                'config': asdict(config),
                'analysis_years': analysis_years,
                'experiment_folder': analysis_logger.run_dir.name
            }
        }
        
        def convert_numpy_types(obj):
        
            if isinstance(obj, (np.integer, np.int64)):
                return int(obj)
            elif isinstance(obj, (np.floating, np.float64)):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_numpy_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            else:
                return obj
        
        # Convert all data to JSON-serializable format
        results_data_clean = convert_numpy_types(results_data)
        
        with open(results_file, 'w') as f:
            json.dump(results_data_clean, f, indent=2)
        
        analysis_logger.log_analysis_step("Results Saved", f"Vulnerability analysis saved to {results_file.name}")
    
    analysis_logger.save_analysis_log()
    
    return {
        'vulnerability_rankings': vulnerability_data,
        'risk_categories': {
            'critical_risk': critical_risk,
            'high_risk': high_risk,
            'medium_risk': medium_risk,
            'low_risk': low_risk
        },
        'vulnerability_insights': {
            'major_importers': major_importers,
            'high_concentration': high_concentration,
            'geopolitical_vulnerable': geopolitical_vulnerable,
            'limited_partners': limited_partners,
            'deteriorating_countries': deteriorating_countries
        },
        'analysis_summary': analysis_summary,
        'model_info': {
            'model_path': str(model_path),
            'config': asdict(config),
            'analysis_years': analysis_years,
            'experiment_folder': analysis_logger.run_dir.name if analysis_logger.run_dir else "unknown"
        }
    }

def display_vulnerability_results(analysis_results: Dict) -> None:

    if not analysis_results:
        return
    
    vulnerability_data = analysis_results['vulnerability_rankings']
    risk_categories = analysis_results['risk_categories']
    insights = analysis_results['vulnerability_insights']
    summary = analysis_results['analysis_summary']
    analysis_year = summary['analysis_year']
    prediction_type = summary.get('prediction_type', 'historical_analysis')
    base_data_year = summary.get('base_data_year', analysis_year)
    
    if prediction_type == 'future_prediction':
        print(f"\nüîÆ WHEAT VULNERABILITY FORECAST - {analysis_year}")
        print(f"üìä Based on data through {base_data_year}")
    else:
        print(f"\nüîç COMPREHENSIVE WHEAT VULNERABILITY ANALYSIS - {analysis_year}")
    print("=" * 80)
    
    print(f"\nüö® TOP 15 MOST VULNERABLE COUNTRIES (Comprehensive Risk Score)")
    print("-" * 100)
    print(f"{'Rank':<4} {'Country':<18} {'Risk':<6} {'Vuln':<6} {'Trend':<6} {'Conc':<6} {'GeoRisk':<8} {'Partners':<8}")
    print("-" * 100)
    
    for i, country_data in enumerate(vulnerability_data[:15]):
        rank = i + 1
        country = country_data['country'][:16]
        risk_score = f"{country_data['comprehensive_risk']:.3f}"
        vuln_score = f"{country_data['predicted_vulnerability']:.3f}"
        trend = f"{country_data['vulnerability_trend']:+.3f}" if abs(country_data['vulnerability_trend']) > 0.001 else "0.000"
        concentration = f"{country_data['import_concentration']:.2f}"
        geo_risk = f"{country_data['geopolitical_risk']:.2f}"
        partners = f"{country_data['import_partners']}"
        
        print(f"{rank:<4} {country:<18} {risk_score:<6} {vuln_score:<6} {trend:<6} {concentration:<6} {geo_risk:<8} {partners:<8}")
    
    print(f"\nüìä RISK CATEGORIES")
    print("-" * 50)
    print(f"üî¥ Critical Risk (>0.8):  {len(risk_categories['critical_risk'])} countries")
    print(f"üü† High Risk (0.6-0.8):   {len(risk_categories['high_risk'])} countries")
    print(f"üü° Medium Risk (0.4-0.6): {len(risk_categories['medium_risk'])} countries")
    print(f"üü¢ Low Risk (<0.4):       {len(risk_categories['low_risk'])} countries")
    
    print(f"\nüí° VULNERABILITY INSIGHTS")
    print("-" * 50)
    print(f"üì¶ Major wheat importers (>1M tons): {len(insights['major_importers'])}")
    print(f"‚ö†Ô∏è  High supply concentration (>70%): {len(insights['high_concentration'])}")
    print(f"üåç Geopolitical vulnerability (>50%): {len(insights['geopolitical_vulnerable'])}")
    print(f"üîó Limited supplier diversity (<3): {len(insights['limited_partners'])}")
    print(f"üìâ Deteriorating food security: {len(insights['deteriorating_countries'])}")
    
    if risk_categories['critical_risk']:
        print(f"\nüö® CRITICAL RISK COUNTRIES:")
        for country_data in risk_categories['critical_risk'][:5]:
            print(f"   ‚Ä¢ {country_data['country']}: Risk {country_data['comprehensive_risk']:.3f} "
                  f"(Vuln: {country_data['predicted_vulnerability']:.3f}, "
                  f"Supply Risk: {country_data['supply_chain_risk']:.3f})")
    
    if insights['geopolitical_vulnerable']:
        print(f"\nüåç HIGH GEOPOLITICAL RISK:")
        for country_data in insights['geopolitical_vulnerable'][:5]:
            suppliers = ", ".join(country_data['major_suppliers']) if country_data['major_suppliers'] else "None"
            print(f"   ‚Ä¢ {country_data['country']}: {country_data['geopolitical_risk']:.2f} "
                  f"(Major suppliers: {suppliers})")
    
    if insights['deteriorating_countries']:
        print(f"\nüìâ DETERIORATING FOOD SECURITY:")
        for country_data in insights['deteriorating_countries'][:5]:
            print(f"   ‚Ä¢ {country_data['country']}: Trend {country_data['vulnerability_trend']:+.3f} "
                  f"(Current: {country_data['predicted_vulnerability']:.3f})")
    
    print(f"\nüìà ANALYSIS SUMMARY")
    print("-" * 50)
    print(f"Total countries analyzed: {summary['total_countries_analyzed']}")
    print(f"Average vulnerability score: {summary['average_vulnerability']:.3f}")
    print(f"Maximum vulnerability score: {summary['max_vulnerability']:.3f}")
    print(f"Countries above risk threshold: {summary['countries_above_baseline']}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'analyze':
        experiment_folder = None
        if len(sys.argv) > 2:
            experiment_folder = sys.argv[2]
            print(f"üéØ Running 2021 vulnerability forecast on specified experiment: {experiment_folder}")
        else:
            print("üéØ Running 2021 vulnerability forecast on latest trained model")
        
        analysis_results = analyze_wheat_vulnerability(experiment_folder=experiment_folder)
        
        if analysis_results:
            display_vulnerability_results(analysis_results)
            
            experiment_folder_name = analysis_results['model_info'].get('experiment_folder', 'unknown')
            print(f"\nüìÅ 2021 vulnerability forecast saved in experiment folder: {experiment_folder_name}")
            print(f"   Check experiments/{experiment_folder_name}/ for all analysis files")
        else:
            print("‚ùå 2021 vulnerability forecast failed")
    else:
        print("üåæ Wheat Vulnerability GNN Analysis")
        print("=" * 50)
        print("üî¨ Starting comprehensive analysis pipeline...")
        print()
        
        results = run_analysis()  # Auto-creates gcn_run_1, gcn_run_2, etc.
        if results:
            print("\n" + "=" * 50)
            print("‚úÖ ANALYSIS COMPLETED SUCCESSFULLY!")
            print("=" * 50)
            print(f"üìä Final Results:")
            print(f"   ‚Ä¢ Test MAE: {results.test_mae:.4f}")
            print(f"   ‚Ä¢ R¬≤ Score: {results.r2_score:.4f}")
            
            if results.baseline_persistence != float('inf'):
                improvement = ((results.baseline_persistence - results.test_mae) / results.baseline_persistence * 100)
                print(f"   ‚Ä¢ vs Persistence Baseline: {improvement:+.1f}%")
            
            if results.baseline_historical != float('inf'):
                improvement_hist = ((results.baseline_historical - results.test_mae) / results.baseline_historical * 100)
                print(f"   ‚Ä¢ vs Historical Baseline: {improvement_hist:+.1f}%")
                
            print(f"   ‚Ä¢ Countries Predicted: {len(results.predictions)}")
            print(f"\nüìÅ Results automatically saved in experiment folder")
            print("   Check experiments/gcn_run_X/ for detailed results")
            
            print(f"\nüîç Running automatic 2021 vulnerability forecast on trained model...")
            print("=" * 60)
            
            try:
                analysis_results = analyze_wheat_vulnerability()
                
                if analysis_results:
                    display_vulnerability_results(analysis_results)
                    
                    experiment_folder_name = analysis_results['model_info'].get('experiment_folder', 'unknown')
                    print(f"\nüìÅ 2021 Vulnerability forecast completed and saved in: {experiment_folder_name}")
                    print(f"   Check experiments/{experiment_folder_name}/ for detailed analysis files")
                else:
                    print("‚ùå Automatic vulnerability forecast failed")
            except Exception as e:
                print(f"‚ùå Automatic vulnerability forecast failed: {e}")
            
            print(f"\nüéØ To run vulnerability forecasts on other models:")
            print(f"   python wheat_vulnerability_gnn.py analyze              # Use latest model for 2021")
            print(f"   python wheat_vulnerability_gnn.py analyze gcn_run_X    # Use specific experiment for 2021")
        else:
            print("\n‚ùå Analysis failed to complete")
            print("Check data files and configuration")
