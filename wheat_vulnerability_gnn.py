import logging
import warnings
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
import json
import datetime
import os

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv, global_mean_pool

logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')

RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

@dataclass
class ModelConfig:
    node_features: int = 12
    hidden_dim: int = 64
    num_gcn_layers: int = 3
    lstm_hidden: int = 32
    dropout_rate: float = 0.2
    
    learning_rate: float = 0.001
    weight_decay: float = 1e-4
    batch_size: int = 16
    epochs: int = 100
    sequence_length: int = 3
    patience: int = 20
    
    model_ensemble_size: int = 3
    use_cpu: bool = True

@dataclass
class ValidationResults:
    cv_mae: float
    test_mae: float
    baseline_persistence: float
    baseline_historical: float
    r2_score: float
    predictions: Dict[str, float]

class ModelLogger:
    def __init__(self, base_dir: str = "experiments"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)
        self.run_dir = None
        self.log_data = []
        self.config_data = {}
        self.analysis_log = []  # New: stores pipeline progress
        
    def create_run_directory(self, config: 'ModelConfig', run_name: str = None):
        if run_name is None:
            # Find the next available run number
            existing_runs = []
            if self.base_dir.exists():
                for item in self.base_dir.iterdir():
                    if item.is_dir() and item.name.startswith('gcn_run_'):
                        try:
                            run_num = int(item.name.split('_')[-1])
                            existing_runs.append(run_num)
                        except ValueError:
                            continue
            
            # Get next run number
            next_run = max(existing_runs, default=0) + 1
            run_name = f"gcn_run_{next_run}"
            
        self.run_dir = self.base_dir / run_name
        self.run_dir.mkdir(exist_ok=True)
        
        self.config_data = asdict(config)
        self.log_data = []
        self.analysis_log = []
        
        # Save config immediately
        self._save_config()
        print(f"Created experiment directory: {self.run_dir}")
        
        # Log experiment start
        self.log_analysis_step("Experiment Started", f"Created directory: {self.run_dir}")
        
    def log_analysis_step(self, step_name: str, details: str):
        """Log analysis pipeline steps"""
        timestamp = datetime.datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] {step_name}: {details}"
        self.analysis_log.append(log_entry)
        
    def log_epoch(self, epoch: int, train_loss: float, val_loss: float = None):
        if val_loss is not None:
            log_line = f"Epoch {epoch:3d}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}"
        else:
            log_line = f"Epoch {epoch:3d}: Train Loss = {train_loss:.6f}"
        self.log_data.append(log_line)
        
    def save_training_log(self):
        if self.run_dir:
            log_path = self.run_dir / "training_log.txt"
            with open(log_path, 'w') as f:
                f.write("Training Log\n")
                f.write("=" * 50 + "\n\n")
                for line in self.log_data:
                    f.write(line + "\n")
                    
    def save_final_metrics(self, results: ValidationResults):
        if self.run_dir:
            metrics = {
                "experiment_info": {
                    "run_name": self.run_dir.name,
                    "timestamp": datetime.datetime.now().isoformat(),
                    "data_source": "FAO Wheat Trade Data"
                },
                "performance_metrics": {
                    "test_mae": float(results.test_mae),
                    "r2_score": float(results.r2_score),
                    "num_predictions": len(results.predictions)
                },
                "baseline_comparison": {
                    "baseline_persistence": float(results.baseline_persistence),
                    "baseline_historical": float(results.baseline_historical),
                    "improvement_vs_persistence_pct": float((results.baseline_persistence - results.test_mae) / results.baseline_persistence * 100) if results.baseline_persistence != float('inf') else 0.0,
                    "improvement_vs_historical_pct": float((results.baseline_historical - results.test_mae) / results.baseline_historical * 100) if results.baseline_historical != float('inf') else 0.0
                },
                "model_files": {
                    "best_model": "best_model.pt",
                    "training_log": "training_log.txt",
                    "analysis_log": "analysis_log.txt",
                    "config": "config.txt"
                }
            }
            
            metrics_path = self.run_dir / "final_metrics.json"
            with open(metrics_path, 'w') as f:
                json.dump(metrics, f, indent=2)
                
    def _save_config(self):
        if self.run_dir:
            config_path = self.run_dir / "config.txt"
            with open(config_path, 'w') as f:
                f.write("Wheat Vulnerability GNN - Experiment Configuration\n")
                f.write("=" * 55 + "\n\n")
                f.write("Model Architecture:\n")
                f.write(f"  node_features: {self.config_data['node_features']}\n")
                f.write(f"  hidden_dim: {self.config_data['hidden_dim']}\n")
                f.write(f"  num_gcn_layers: {self.config_data['num_gcn_layers']}\n")
                f.write(f"  lstm_hidden: {self.config_data['lstm_hidden']}\n")
                f.write(f"  dropout_rate: {self.config_data['dropout_rate']}\n\n")
                
                f.write("Training Parameters:\n")
                f.write(f"  learning_rate: {self.config_data['learning_rate']}\n")
                f.write(f"  weight_decay: {self.config_data['weight_decay']}\n")
                f.write(f"  batch_size: {self.config_data['batch_size']}\n")
                f.write(f"  epochs: {self.config_data['epochs']}\n")
                f.write(f"  sequence_length: {self.config_data['sequence_length']}\n")
                f.write(f"  patience: {self.config_data['patience']}\n\n")
                
                f.write("Other Settings:\n")
                f.write(f"  model_ensemble_size: {self.config_data['model_ensemble_size']}\n")
                f.write(f"  use_cpu: {self.config_data['use_cpu']}\n")
                f.write(f"\nExperiment Run: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    
    def get_model_path(self) -> str:
        if self.run_dir:
            return str(self.run_dir / "best_model.pt")
        return "best_model.pt"
    
    def save_final_model(self, model, config: 'ModelConfig', final_metrics: dict):
        """Save the final trained model with metadata"""
        if self.run_dir:
            model_path = self.run_dir / "best_model.pt"
            torch.save({
                'model_state_dict': model.state_dict(),
                'config': asdict(config),
                'final_metrics': final_metrics,
                'model_architecture': str(model),
                'timestamp': datetime.datetime.now().isoformat()
            }, model_path)
            print(f"Model saved: {model_path}")
            
    def save_analysis_log(self):
        """Save the analysis pipeline log"""
        if self.run_dir:
            log_path = self.run_dir / "analysis_log.txt"
            with open(log_path, 'w') as f:
                f.write("Wheat Vulnerability GNN - Analysis Pipeline Log\n")
                f.write("=" * 60 + "\n\n")
                f.write(f"Experiment: {self.run_dir.name}\n")
                f.write(f"Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write("Pipeline Steps:\n")
                f.write("-" * 30 + "\n")
                for entry in self.analysis_log:
                    f.write(entry + "\n")
                f.write("\n" + "=" * 60 + "\n")

class DataProcessor:
    
    def __init__(self, data_path: Union[str, Path] = 'data/wheat_trade_data.csv'):
        self.data_path = Path(data_path)
        self.scaler = StandardScaler()
        self._validate_data_path()
    
    def _validate_data_path(self) -> None:
        if not self.data_path.exists():
            raise FileNotFoundError(f"Data file not found: {self.data_path}")
    
    def load_wheat_data(self) -> pd.DataFrame:
        df = pd.read_csv(self.data_path)
        
        required_cols = ['Year', 'Reporter Countries', 'Partner Countries', 'Value', 'Element Code']
        missing_cols = set(required_cols) - set(df.columns)
        if missing_cols:
            raise ValueError(f"Missing required columns: {missing_cols}")
        
        imports = (df[df['Element Code'] == 5610]
                  [['Year', 'Reporter Countries', 'Partner Countries', 'Value']]
                  .copy())
        
        imports.columns = ['year', 'importer', 'exporter', 'value']
        
        imports = (imports
                  .query('year >= 2000 and year <= 2020')
                  .query('value > 0')
                  .dropna()
                  .groupby(['year', 'importer', 'exporter'])['value']
                  .sum()
                  .reset_index())
        
        return imports
    
    def estimate_consumption_baselines(self, data: pd.DataFrame = None) -> Dict[str, float]:
        if data is None:
            data = self.load_wheat_data()
        
        yearly_imports = data.groupby(['year', 'importer'])['value'].sum().reset_index()
        yearly_exports = data.groupby(['year', 'exporter'])['value'].sum().reset_index()
        yearly_exports.columns = ['year', 'country', 'exports']
        yearly_imports.columns = ['year', 'country', 'imports']
        
        trade_balance = pd.merge(yearly_imports, yearly_exports, on=['year', 'country'], how='outer').fillna(0)
        trade_balance['net_imports'] = trade_balance['imports'] - trade_balance['exports']
        
        consumption = (trade_balance
                      .groupby('country')['net_imports']
                      .agg(['median', 'count'])
                      .reset_index())
        
        consumption = consumption.query('count >= 5 and median > 0')
        
        baseline_dict = dict(zip(consumption['country'], consumption['median']))
        
        return baseline_dict

    def calculate_vulnerability_targets(self, data: pd.DataFrame = None, 
                                      consumption_baselines: Dict[str, float] = None) -> Dict[int, Dict[str, float]]:
        if data is None:
            data = self.load_wheat_data()
        if consumption_baselines is None:
            consumption_baselines = self.estimate_consumption_baselines(data)
        
        targets = {}
        for year in sorted(data['year'].unique()):
            year_data = data[data['year'] == year]
            year_imports = year_data.groupby('importer')['value'].sum()
            year_targets = {}
            
            for country, baseline in consumption_baselines.items():
                actual_imports = year_imports.get(country, 0)
                vulnerability = max(0, min(1, 1 - (actual_imports / baseline)))
                year_targets[country] = vulnerability
            
            targets[year] = year_targets
        
        return targets

class GraphBuilder:
    
    def __init__(self, config: ModelConfig):
        self.config = config
    
    def build_temporal_graph(self, data: pd.DataFrame, year: int, countries: List[str]) -> Data:
        year_data = data[data['year'] == year]
        country_to_idx = {country: idx for idx, country in enumerate(countries)}
        n_countries = len(countries)
        
        if year_data.empty:
            return Data(
                x=torch.zeros(n_countries, self.config.node_features),
                edge_index=torch.empty(2, 0, dtype=torch.long),
                edge_attr=torch.empty(0, 1)
            )
        
        edge_sources, edge_targets, edge_weights = [], [], []
        for _, row in year_data.iterrows():
            src_idx = country_to_idx.get(row['exporter'])
            tgt_idx = country_to_idx.get(row['importer'])
            
            if src_idx is not None and tgt_idx is not None:
                edge_sources.append(src_idx)
                edge_targets.append(tgt_idx)
                edge_weights.append(np.log1p(row['value']))
        
        node_features = self._compute_node_features(data, year, countries, country_to_idx)
        
        return Data(
            x=torch.tensor(node_features, dtype=torch.float32),
            edge_index=torch.tensor([edge_sources, edge_targets], dtype=torch.long) if edge_sources else torch.empty(2, 0, dtype=torch.long),
            edge_attr=torch.tensor(edge_weights, dtype=torch.float32).unsqueeze(1) if edge_weights else torch.empty(0, 1)
        )
    
    def _compute_node_features(self, data: pd.DataFrame, year: int, countries: List[str], 
                             country_to_idx: Dict[str, int]) -> np.ndarray:
        features = np.zeros((len(countries), self.config.node_features))
        
        for idx, country in enumerate(countries):
            feature_years = [y for y in range(year-3, year+1) if y >= data['year'].min()]
            
            import_history = []
            export_history = []
            partner_counts = []
            
            for fy in feature_years:
                year_data = data[data['year'] == fy]
                imports = year_data[year_data['importer'] == country]['value'].sum()
                exports = year_data[year_data['exporter'] == country]['value'].sum()
                
                import_partners = len(year_data[year_data['importer'] == country]['exporter'].unique())
                export_partners = len(year_data[year_data['exporter'] == country]['importer'].unique())
                
                import_history.append(np.log1p(imports))
                export_history.append(np.log1p(exports))
                partner_counts.append(import_partners + export_partners)
            
            current_imports = import_history[-1] if import_history else 0
            current_exports = export_history[-1] if export_history else 0
            current_partners = partner_counts[-1] if partner_counts else 0
            
            import_trend = np.polyfit(range(len(import_history)), import_history, 1)[0] if len(import_history) > 1 else 0
            export_trend = np.polyfit(range(len(export_history)), export_history, 1)[0] if len(export_history) > 1 else 0
            import_volatility = np.std(import_history) if len(import_history) > 1 else 0
            export_volatility = np.std(export_history) if len(export_history) > 1 else 0
            
            trade_balance = current_exports - current_imports
            trade_openness = current_imports + current_exports
            
            import_concentration = self._calculate_concentration(data, year, country, 'importer')
            export_concentration = self._calculate_concentration(data, year, country, 'exporter')
            centrality_score = self._calculate_centrality(data, year, country)
            
            features[idx] = [
                current_imports,
                current_exports,
                current_partners,
                trade_balance,
                import_trend,
                export_trend,
                import_volatility,
                export_volatility,
                trade_openness,
                import_concentration,
                export_concentration,
                centrality_score
            ]
        
        return features
    
    def _calculate_concentration(self, data: pd.DataFrame, year: int, country: str, role: str) -> float:
        year_data = data[data['year'] == year]
        if role == 'importer':
            country_data = year_data[year_data['importer'] == country]
        else:
            country_data = year_data[year_data['exporter'] == country]
        
        total_trade = country_data['value'].sum()
        if total_trade == 0:
            return 0
        
        shares = (country_data['value'] / total_trade) ** 2
        return shares.sum()
    
    def _calculate_centrality(self, data: pd.DataFrame, year: int, country: str) -> float:
        year_data = data[data['year'] == year]
        countries = list(set(year_data['importer'].tolist() + year_data['exporter'].tolist()))
        
        if country not in countries:
            return 0
        
        in_degree = len(year_data[year_data['importer'] == country]['exporter'].unique())
        out_degree = len(year_data[year_data['exporter'] == country]['importer'].unique())
        
        return (in_degree + out_degree) / max(len(countries) - 1, 1)

class VulnerabilityGNN(nn.Module):
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        
        self.gcn_layers = nn.ModuleList([
            GCNConv(config.node_features if i == 0 else config.hidden_dim, config.hidden_dim)
            for i in range(config.num_gcn_layers)
        ])
        
        self.lstm = nn.LSTM(
            config.hidden_dim, 
            config.lstm_hidden, 
            batch_first=True, 
            dropout=config.dropout_rate
        )
        
        self.predictor = nn.Sequential(
            nn.Linear(config.lstm_hidden, config.hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(config.dropout_rate),
            nn.Linear(config.hidden_dim // 2, 1),
            nn.Sigmoid()
        )
        
        self.dropout = nn.Dropout(config.dropout_rate)
    
    def forward(self, graph_sequence: List[Data]) -> torch.Tensor:
        graph_embeddings = []
        
        for graph in graph_sequence:
            x = graph.x
            
            for gcn in self.gcn_layers:
                x = gcn(x, graph.edge_index)
                x = torch.relu(x)
                x = self.dropout(x)
            
            x_pooled = global_mean_pool(x, torch.zeros(x.size(0), dtype=torch.long))
            graph_embeddings.append(x_pooled)
        
        if len(graph_embeddings) > 1:
            sequence = torch.stack(graph_embeddings, dim=1)
            lstm_out, _ = self.lstm(sequence)
            final_embedding = lstm_out[:, -1, :]
        else:
            final_embedding = graph_embeddings[0]
        
        vulnerability_pred = self.predictor(final_embedding)
        
        return vulnerability_pred

class ModelTrainer:
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.device = torch.device('cpu')
        self.model_logger = ModelLogger(base_dir=r"C:\Users\Windows 11\Desktop\Journal\github_repo\experiments")
    
    def train_model(self, train_sequences: List[Tuple], val_sequences: List[Tuple] = None, experiment_name: str = None) -> VulnerabilityGNN:
        self.model_logger.create_run_directory(self.config, experiment_name)
        
        model = VulnerabilityGNN(self.config).to(self.device)
        
        optimizer = torch.optim.AdamW(
            model.parameters(), 
            lr=self.config.learning_rate, 
            weight_decay=self.config.weight_decay
        )
        
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', patience=self.config.patience // 3, factor=0.5
        )
        
        criterion = nn.MSELoss()
        best_val_loss = float('inf')
        patience_counter = 0
        best_model_path = self.model_logger.get_model_path()
        
        for epoch in range(self.config.epochs):
            model.train()
            train_loss = 0.0
            
            # Progress indicator for training
            if epoch % 10 == 0 or epoch == self.config.epochs - 1:
                progress = (epoch + 1) / self.config.epochs * 100
                print(f"Training Progress: {progress:.1f}% (Epoch {epoch + 1}/{self.config.epochs})")
            
            for seq, target in train_sequences:
                optimizer.zero_grad()
                predictions = model(seq)
                loss = criterion(predictions, target.to(self.device))
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
            
            train_loss /= len(train_sequences)
            
            val_loss = 0.0
            if val_sequences:
                model.eval()
                with torch.no_grad():
                    for seq, target in val_sequences:
                        predictions = model(seq)
                        loss = criterion(predictions, target.to(self.device))
                        val_loss += loss.item()
                
                val_loss /= len(val_sequences)
                scheduler.step(val_loss)
                
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                    torch.save({
                        'model_state_dict': model.state_dict(),
                        'config': asdict(self.config),
                        'epoch': epoch,
                        'train_loss': train_loss,
                        'val_loss': val_loss
                    }, best_model_path)
                    if epoch % 10 == 0 or epoch == self.config.epochs - 1:
                        print(f"  → New best model saved! Val Loss: {val_loss:.6f}")
                else:
                    patience_counter += 1
                
                if patience_counter >= self.config.patience:
                    print(f"  → Early stopping at epoch {epoch + 1} (patience reached)")
                    break
                    
                # Show loss every 10 epochs
                if epoch % 10 == 0 or epoch == self.config.epochs - 1:
                    print(f"  → Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")
            else:
                # No validation data - save based on training loss improvement
                if train_loss < best_val_loss:
                    best_val_loss = train_loss
                    torch.save({
                        'model_state_dict': model.state_dict(),
                        'config': asdict(self.config),
                        'epoch': epoch,
                        'train_loss': train_loss,
                        'val_loss': 0.0
                    }, best_model_path)
                    if epoch % 10 == 0 or epoch == self.config.epochs - 1:
                        print(f"  → New best model saved! Train Loss: {train_loss:.6f}")
                
                # Show loss every 10 epochs
                if epoch % 10 == 0 or epoch == self.config.epochs - 1:
                    print(f"  → Train Loss: {train_loss:.6f}")
            
            self.model_logger.log_epoch(epoch, train_loss, val_loss)
        
        # Always save final model if no model was saved during training
        if not os.path.exists(best_model_path):
            torch.save({
                'model_state_dict': model.state_dict(),
                'config': asdict(self.config),
                'epoch': self.config.epochs - 1,
                'train_loss': train_loss,
                'val_loss': val_loss
            }, best_model_path)
        
        # Load best model if it exists
        if os.path.exists(best_model_path):
            checkpoint = torch.load(best_model_path)
            model.load_state_dict(checkpoint['model_state_dict'])
        
        return model
    
    def evaluate_model(self, model: VulnerabilityGNN, test_sequences: List[Tuple]) -> Tuple[float, np.ndarray, np.ndarray]:
        model.eval()
        predictions, actuals = [], []
        
        with torch.no_grad():
            for seq, target in test_sequences:
                pred = model(seq)
                predictions.append(pred.cpu().numpy())
                actuals.append(target.numpy())
        
        if predictions:
            pred_array = np.concatenate(predictions, axis=0).flatten()
            actual_array = np.concatenate(actuals, axis=0).flatten()
            
            min_len = min(len(pred_array), len(actual_array))
            pred_array = pred_array[:min_len]
            actual_array = actual_array[:min_len]
            
            mae = mean_absolute_error(actual_array, pred_array)
            return mae, pred_array, actual_array
        else:
            return float('inf'), np.array([]), np.array([])

def prepare_sequences(data: pd.DataFrame, targets: Dict[int, Dict[str, float]], 
                     countries: List[str], years: List[int], seq_len: int,
                     graph_builder: GraphBuilder) -> Tuple[List, List]:
    sequences, sequence_targets = [], []
    
    for i in range(seq_len, len(years)):
        input_years = years[i-seq_len:i]
        target_year = years[i]
        
        if target_year not in targets:
            continue
        
        graphs = []
        for year in input_years:
            graph = graph_builder.build_temporal_graph(data, year, countries)
            graphs.append(graph)
        
        target_dict = targets[target_year]
        target_tensor = torch.zeros(len(countries), 1)
        
        for j, country in enumerate(countries):
            if country in target_dict:
                target_tensor[j, 0] = target_dict[country]
        
        sequences.append(graphs)
        sequence_targets.append(target_tensor)
    
    return sequences, sequence_targets

def baseline_comparison(data: pd.DataFrame, targets: Dict[int, Dict[str, float]], 
                       countries: List[str]) -> Dict[str, float]:
    test_years = sorted([y for y in targets.keys() if y >= 2018])
    
    if len(test_years) < 2:
        return {'persistence': float('inf'), 'historical_avg': float('inf')}
    
    all_vulnerabilities = [v for year_targets in targets.values() for v in year_targets.values()]
    hist_avg = np.mean(all_vulnerabilities)
    
    persistence_errors, hist_avg_errors = [], []
    
    for year in test_years[1:]:
        if year in targets and year-1 in targets:
            current = np.array([targets[year].get(c, 0) for c in countries])
            previous = np.array([targets[year-1].get(c, 0) for c in countries])
            hist_avg_pred = np.full_like(current, hist_avg)
            
            persistence_errors.append(mean_absolute_error(current, previous))
            hist_avg_errors.append(mean_absolute_error(current, hist_avg_pred))
    
    return {
        'persistence': np.mean(persistence_errors) if persistence_errors else float('inf'),
        'historical_avg': np.mean(hist_avg_errors) if hist_avg_errors else float('inf')
    }

def run_analysis(experiment_name: str = None) -> Optional[ValidationResults]:
    config = ModelConfig()
    data_processor = DataProcessor()
    graph_builder = GraphBuilder(config)
    trainer = ModelTrainer(config)
    
    try:
        print("📊 Loading and processing wheat trade data...")
        data = data_processor.load_wheat_data()
        print(f"   → Loaded {len(data)} trade records")
        trainer.model_logger.log_analysis_step("Data Loading", f"Loaded {len(data)} trade records from wheat trade data")
        
        print("🔍 Calculating consumption baselines...")
        consumption_baselines = data_processor.estimate_consumption_baselines(data)
        print(f"   → Estimated baselines for {len(consumption_baselines)} countries")
        trainer.model_logger.log_analysis_step("Baseline Calculation", f"Estimated baselines for {len(consumption_baselines)} countries")
        
        print("🎯 Computing vulnerability targets...")
        targets = data_processor.calculate_vulnerability_targets(data, consumption_baselines)
        countries = list(consumption_baselines.keys())
        print(f"   → Computed targets for {len(targets)} years")
        trainer.model_logger.log_analysis_step("Target Computation", f"Computed vulnerability targets for {len(targets)} years across {len(countries)} countries")
        
        print("📈 Computing baseline comparisons...")
        baseline_results = baseline_comparison(data, targets, countries)
        trainer.model_logger.log_analysis_step("Baseline Comparison", f"Persistence MAE: {baseline_results['persistence']:.4f}, Historical MAE: {baseline_results['historical_avg']:.4f}")
        
        all_years = sorted(data['year'].unique())
        train_years = [y for y in all_years if y <= 2015]
        test_years = [y for y in all_years if y >= 2016]
        
        print(f"🔄 Preparing sequences...")
        print(f"   → Training years: {train_years[0]}-{train_years[-1]} ({len(train_years)} years)")
        print(f"   → Test years: {test_years[0]}-{test_years[-1]} ({len(test_years)} years)")
        trainer.model_logger.log_analysis_step("Data Split", f"Training: {train_years[0]}-{train_years[-1]} ({len(train_years)} years), Test: {test_years[0]}-{test_years[-1]} ({len(test_years)} years)")
        
        train_seq, train_tgt = prepare_sequences(data, targets, countries, train_years, config.sequence_length, graph_builder)
        val_seq, val_tgt = prepare_sequences(data, targets, countries, train_years[-3:], config.sequence_length, graph_builder)
        test_seq, test_tgt = prepare_sequences(data, targets, countries, test_years, config.sequence_length, graph_builder)
        
        if not train_seq:
            print("❌ No training sequences generated!")
            trainer.model_logger.log_analysis_step("ERROR", "No training sequences generated - analysis failed")
            return None
        
        print(f"   → Generated {len(train_seq)} training sequences")
        print(f"   → Generated {len(val_seq) if val_seq else 0} validation sequences")
        print(f"   → Generated {len(test_seq)} test sequences")
        trainer.model_logger.log_analysis_step("Sequence Generation", f"Training: {len(train_seq)}, Validation: {len(val_seq) if val_seq else 0}, Test: {len(test_seq)} sequences")
        
        train_data = list(zip(train_seq, train_tgt))
        val_data = list(zip(val_seq, val_tgt)) if val_seq else None
        test_data = list(zip(test_seq, test_tgt)) if test_seq else []
        
        print(f"\n🚀 Starting model training...")
        print(f"   → Model: {config.num_gcn_layers}-layer GCN + LSTM")
        print(f"   → Learning rate: {config.learning_rate}")
        print(f"   → Max epochs: {config.epochs}")
        print("-" * 50)
        trainer.model_logger.log_analysis_step("Training Start", f"Model: {config.num_gcn_layers}-layer GCN+LSTM, LR: {config.learning_rate}, Max epochs: {config.epochs}")
        
        model = trainer.train_model(train_data, val_data, experiment_name)
        
        print("\n🧪 Evaluating model on test data...")
        if test_data:
            test_mae, predictions, actuals = trainer.evaluate_model(model, test_data)
            
            ss_res = np.sum((actuals - predictions) ** 2)
            ss_tot = np.sum((actuals - np.mean(actuals)) ** 2)
            r2_score = 1 - (ss_res / (ss_tot + 1e-8))
            
            print(f"   → Test predictions generated: {len(predictions)}")
            trainer.model_logger.log_analysis_step("Model Evaluation", f"Test MAE: {test_mae:.4f}, R²: {r2_score:.4f}, Predictions: {len(predictions)}")
        else:
            test_mae, r2_score = float('inf'), 0.0
            predictions = np.array([])
            print("   → No test data available")
            trainer.model_logger.log_analysis_step("Model Evaluation", "No test data available")
        
        country_predictions = {}
        if len(predictions) > 0:
            country_predictions = {countries[i % len(countries)]: float(predictions[i]) 
                                 for i in range(min(len(countries), len(predictions)))}
        
        results = ValidationResults(
            cv_mae=0.0,
            test_mae=test_mae,
            baseline_persistence=baseline_results['persistence'],
            baseline_historical=baseline_results['historical_avg'],
            r2_score=r2_score,
            predictions=country_predictions
        )
        
        print("\n💾 Saving results...")
        # Save all training artifacts including the final model
        trainer.model_logger.save_training_log()
        trainer.model_logger.save_final_metrics(results)
        trainer.model_logger.save_final_model(model, config, {
            'test_mae': test_mae,
            'r2_score': r2_score,
            'baseline_persistence': baseline_results['persistence'],
            'baseline_historical': baseline_results['historical_avg']
        })
        trainer.model_logger.save_analysis_log()  # Save the new analysis log
        trainer.model_logger.log_analysis_step("Results Saved", f"All files saved to {trainer.model_logger.run_dir}")
        print("   → All files saved successfully!")
        
        return results
        
    except Exception as e:
        raise

if __name__ == "__main__":
    # Each run automatically creates a new numbered experiment folder
    print("🌾 Wheat Vulnerability GNN Analysis")
    print("=" * 50)
    print("🔬 Starting comprehensive analysis pipeline...")
    print()
    
    results = run_analysis()  # Auto-creates gcn_run_1, gcn_run_2, etc.
    if results:
        print("\n" + "=" * 50)
        print("✅ ANALYSIS COMPLETED SUCCESSFULLY!")
        print("=" * 50)
        print(f"📊 Final Results:")
        print(f"   • Test MAE: {results.test_mae:.4f}")
        print(f"   • R² Score: {results.r2_score:.4f}")
        
        if results.baseline_persistence != float('inf'):
            improvement = ((results.baseline_persistence - results.test_mae) / results.baseline_persistence * 100)
            print(f"   • vs Persistence Baseline: {improvement:+.1f}%")
        
        if results.baseline_historical != float('inf'):
            improvement_hist = ((results.baseline_historical - results.test_mae) / results.baseline_historical * 100)
            print(f"   • vs Historical Baseline: {improvement_hist:+.1f}%")
            
        print(f"   • Countries Predicted: {len(results.predictions)}")
        print(f"\n📁 Results automatically saved in experiment folder")
        print("   Check experiments/gcn_run_X/ for detailed results")
    else:
        print("\n❌ Analysis failed to complete")
        print("Check data files and configuration")
