import logging
import warnings
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv, global_mean_pool

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore', category=UserWarning)

RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

@dataclass
class ModelConfig:
    node_features: int = 12
    hidden_dim: int = 64
    num_gcn_layers: int = 3
    lstm_hidden: int = 32
    dropout_rate: float = 0.2
    
    learning_rate: float = 0.001
    weight_decay: float = 1e-4
    batch_size: int = 16
    epochs: int = 100
    sequence_length: int = 3
    patience: int = 20
    
    model_ensemble_size: int = 3
    use_cpu: bool = True

@dataclass
class ValidationResults:
    cv_mae: float
    test_mae: float
    baseline_persistence: float
    baseline_historical: float
    r2_score: float
    predictions: Dict[str, float]

class DataProcessor:
    
    def __init__(self, data_path: Union[str, Path] = 'data/wheat_trade_data.csv'):
        self.data_path = Path(data_path)
        self.scaler = StandardScaler()
        self._validate_data_path()
    
    def _validate_data_path(self) -> None:
        if not self.data_path.exists():
            raise FileNotFoundError(f"Data file not found: {self.data_path}")
    
    def load_wheat_data(self) -> pd.DataFrame:
        df = pd.read_csv(self.data_path)
        logger.info(f"Loaded {len(df)} raw records")
        
        required_cols = ['Year', 'Reporter Countries', 'Partner Countries', 'Value', 'Element Code']
        missing_cols = set(required_cols) - set(df.columns)
        if missing_cols:
            raise ValueError(f"Missing required columns: {missing_cols}")
        
        imports = (df[df['Element Code'] == 5610]
                  [['Year', 'Reporter Countries', 'Partner Countries', 'Value']]
                  .copy())
        
        imports.columns = ['year', 'importer', 'exporter', 'value']
        
        imports = (imports
                  .query('year >= 2000 and year <= 2020')
                  .query('value > 0')
                  .dropna()
                  .groupby(['year', 'importer', 'exporter'])['value']
                  .sum()
                  .reset_index())
        
        logger.info(f"Processed to {len(imports)} clean trade records")
        return imports
    
    def estimate_consumption_baselines(self, data: pd.DataFrame = None) -> Dict[str, float]:
        if data is None:
            data = self.load_wheat_data()
        
        yearly_imports = data.groupby(['year', 'importer'])['value'].sum().reset_index()
        yearly_exports = data.groupby(['year', 'exporter'])['value'].sum().reset_index()
        yearly_exports.columns = ['year', 'country', 'exports']
        yearly_imports.columns = ['year', 'country', 'imports']
        
        trade_balance = pd.merge(yearly_imports, yearly_exports, on=['year', 'country'], how='outer').fillna(0)
        trade_balance['net_imports'] = trade_balance['imports'] - trade_balance['exports']
        
        consumption = (trade_balance
                      .groupby('country')['net_imports']
                      .agg(['median', 'count'])
                      .reset_index())
        
        consumption = consumption.query('count >= 5 and median > 0')
        
        baseline_dict = dict(zip(consumption['country'], consumption['median']))
        logger.info(f"Calculated baselines for {len(baseline_dict)} countries")
        
        return baseline_dict

    def calculate_vulnerability_targets(self, data: pd.DataFrame = None, 
                                      consumption_baselines: Dict[str, float] = None) -> Dict[int, Dict[str, float]]:
        if data is None:
            data = self.load_wheat_data()
        if consumption_baselines is None:
            consumption_baselines = self.estimate_consumption_baselines(data)
        
        targets = {}
        for year in sorted(data['year'].unique()):
            year_data = data[data['year'] == year]
            year_imports = year_data.groupby('importer')['value'].sum()
            year_targets = {}
            
            for country, baseline in consumption_baselines.items():
                actual_imports = year_imports.get(country, 0)
                vulnerability = max(0, min(1, 1 - (actual_imports / baseline)))
                year_targets[country] = vulnerability
            
            targets[year] = year_targets
        
        logger.info(f"Calculated vulnerability targets for {len(targets)} years")
        return targets

class GraphBuilder:
    
    def __init__(self, config: ModelConfig):
        self.config = config
    
    def build_temporal_graph(self, data: pd.DataFrame, year: int, countries: List[str]) -> Data:
        year_data = data[data['year'] == year]
        country_to_idx = {country: idx for idx, country in enumerate(countries)}
        n_countries = len(countries)
        
        if year_data.empty:
            return Data(
                x=torch.zeros(n_countries, self.config.node_features),
                edge_index=torch.empty(2, 0, dtype=torch.long),
                edge_attr=torch.empty(0, 1)
            )
        
        edge_sources, edge_targets, edge_weights = [], [], []
        for _, row in year_data.iterrows():
            src_idx = country_to_idx.get(row['exporter'])
            tgt_idx = country_to_idx.get(row['importer'])
            
            if src_idx is not None and tgt_idx is not None:
                edge_sources.append(src_idx)
                edge_targets.append(tgt_idx)
                edge_weights.append(np.log1p(row['value']))
        
        node_features = self._compute_node_features(data, year, countries, country_to_idx)
        
        return Data(
            x=torch.tensor(node_features, dtype=torch.float32),
            edge_index=torch.tensor([edge_sources, edge_targets], dtype=torch.long) if edge_sources else torch.empty(2, 0, dtype=torch.long),
            edge_attr=torch.tensor(edge_weights, dtype=torch.float32).unsqueeze(1) if edge_weights else torch.empty(0, 1)
        )
    
    def _compute_node_features(self, data: pd.DataFrame, year: int, countries: List[str], 
                             country_to_idx: Dict[str, int]) -> np.ndarray:
        features = np.zeros((len(countries), self.config.node_features))
        
        for idx, country in enumerate(countries):
            feature_years = [y for y in range(year-3, year+1) if y >= data['year'].min()]
            
            import_history = []
            export_history = []
            partner_counts = []
            
            for fy in feature_years:
                year_data = data[data['year'] == fy]
                imports = year_data[year_data['importer'] == country]['value'].sum()
                exports = year_data[year_data['exporter'] == country]['value'].sum()
                
                import_partners = len(year_data[year_data['importer'] == country]['exporter'].unique())
                export_partners = len(year_data[year_data['exporter'] == country]['importer'].unique())
                
                import_history.append(np.log1p(imports))
                export_history.append(np.log1p(exports))
                partner_counts.append(import_partners + export_partners)
            
            current_imports = import_history[-1] if import_history else 0
            current_exports = export_history[-1] if export_history else 0
            current_partners = partner_counts[-1] if partner_counts else 0
            
            import_trend = np.polyfit(range(len(import_history)), import_history, 1)[0] if len(import_history) > 1 else 0
            export_trend = np.polyfit(range(len(export_history)), export_history, 1)[0] if len(export_history) > 1 else 0
            import_volatility = np.std(import_history) if len(import_history) > 1 else 0
            export_volatility = np.std(export_history) if len(export_history) > 1 else 0
            
            trade_balance = current_exports - current_imports
            trade_openness = current_imports + current_exports
            
            import_concentration = self._calculate_concentration(data, year, country, 'importer')
            export_concentration = self._calculate_concentration(data, year, country, 'exporter')
            centrality_score = self._calculate_centrality(data, year, country)
            
            features[idx] = [
                current_imports,
                current_exports,
                current_partners,
                trade_balance,
                import_trend,
                export_trend,
                import_volatility,
                export_volatility,
                trade_openness,
                import_concentration,
                export_concentration,
                centrality_score
            ]
        
        return features
    
    def _calculate_concentration(self, data: pd.DataFrame, year: int, country: str, role: str) -> float:
        year_data = data[data['year'] == year]
        if role == 'importer':
            country_data = year_data[year_data['importer'] == country]
        else:
            country_data = year_data[year_data['exporter'] == country]
        
        total_trade = country_data['value'].sum()
        if total_trade == 0:
            return 0
        
        shares = (country_data['value'] / total_trade) ** 2
        return shares.sum()
    
    def _calculate_centrality(self, data: pd.DataFrame, year: int, country: str) -> float:
        year_data = data[data['year'] == year]
        countries = list(set(year_data['importer'].tolist() + year_data['exporter'].tolist()))
        
        if country not in countries:
            return 0
        
        in_degree = len(year_data[year_data['importer'] == country]['exporter'].unique())
        out_degree = len(year_data[year_data['exporter'] == country]['importer'].unique())
        
        return (in_degree + out_degree) / max(len(countries) - 1, 1)

class VulnerabilityGNN(nn.Module):
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        
        self.gcn_layers = nn.ModuleList([
            GCNConv(config.node_features if i == 0 else config.hidden_dim, config.hidden_dim)
            for i in range(config.num_gcn_layers)
        ])
        
        self.lstm = nn.LSTM(
            config.hidden_dim, 
            config.lstm_hidden, 
            batch_first=True, 
            dropout=config.dropout_rate
        )
        
        self.predictor = nn.Sequential(
            nn.Linear(config.lstm_hidden, config.hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(config.dropout_rate),
            nn.Linear(config.hidden_dim // 2, 1),
            nn.Sigmoid()
        )
        
        self.dropout = nn.Dropout(config.dropout_rate)
    
    def forward(self, graph_sequence: List[Data]) -> torch.Tensor:
        graph_embeddings = []
        
        for graph in graph_sequence:
            x = graph.x
            
            for gcn in self.gcn_layers:
                x = gcn(x, graph.edge_index)
                x = torch.relu(x)
                x = self.dropout(x)
            
            x_pooled = global_mean_pool(x, torch.zeros(x.size(0), dtype=torch.long))
            graph_embeddings.append(x_pooled)
        
        if len(graph_embeddings) > 1:
            sequence = torch.stack(graph_embeddings, dim=1)
            lstm_out, _ = self.lstm(sequence)
            final_embedding = lstm_out[:, -1, :]
        else:
            final_embedding = graph_embeddings[0]
        
        vulnerability_pred = self.predictor(final_embedding)
        
        return vulnerability_pred

class ModelTrainer:
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.device = torch.device('cpu')
        logger.info("Using CPU for training")
    
    def train_model(self, train_sequences: List[Tuple], val_sequences: List[Tuple] = None) -> VulnerabilityGNN:
        model = VulnerabilityGNN(self.config).to(self.device)
        
        optimizer = torch.optim.AdamW(
            model.parameters(), 
            lr=self.config.learning_rate, 
            weight_decay=self.config.weight_decay
        )
        
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', patience=self.config.patience // 3, factor=0.5
        )
        
        criterion = nn.MSELoss()
        best_val_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(self.config.epochs):
            model.train()
            train_loss = 0.0
            
            for seq, target in train_sequences:
                optimizer.zero_grad()
                predictions = model(seq)
                loss = criterion(predictions, target.to(self.device))
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
            
            train_loss /= len(train_sequences)
            
            val_loss = 0.0
            if val_sequences:
                model.eval()
                with torch.no_grad():
                    for seq, target in val_sequences:
                        predictions = model(seq)
                        loss = criterion(predictions, target.to(self.device))
                        val_loss += loss.item()
                
                val_loss /= len(val_sequences)
                scheduler.step(val_loss)
                
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                    torch.save(model.state_dict(), 'best_model.pth')
                else:
                    patience_counter += 1
                
                if patience_counter >= self.config.patience:
                    logger.info(f"Early stopping at epoch {epoch + 1}")
                    break
            
            if epoch % 20 == 0:
                logger.info(f"Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
        
        if val_sequences:
            model.load_state_dict(torch.load('best_model.pth'))
        
        return model
    
    def evaluate_model(self, model: VulnerabilityGNN, test_sequences: List[Tuple]) -> Tuple[float, np.ndarray, np.ndarray]:
        model.eval()
        predictions, actuals = [], []
        
        with torch.no_grad():
            for seq, target in test_sequences:
                pred = model(seq)
                predictions.append(pred.cpu().numpy())
                actuals.append(target.numpy())
        
        if predictions:
            pred_array = np.concatenate(predictions, axis=0).flatten()
            actual_array = np.concatenate(actuals, axis=0).flatten()
            
            min_len = min(len(pred_array), len(actual_array))
            pred_array = pred_array[:min_len]
            actual_array = actual_array[:min_len]
            
            mae = mean_absolute_error(actual_array, pred_array)
            return mae, pred_array, actual_array
        else:
            return float('inf'), np.array([]), np.array([])

def prepare_sequences(data: pd.DataFrame, targets: Dict[int, Dict[str, float]], 
                     countries: List[str], years: List[int], seq_len: int,
                     graph_builder: GraphBuilder) -> Tuple[List, List]:
    sequences, sequence_targets = [], []
    
    for i in range(seq_len, len(years)):
        input_years = years[i-seq_len:i]
        target_year = years[i]
        
        if target_year not in targets:
            continue
        
        graphs = []
        for year in input_years:
            graph = graph_builder.build_temporal_graph(data, year, countries)
            graphs.append(graph)
        
        target_dict = targets[target_year]
        target_tensor = torch.zeros(len(countries), 1)
        
        for j, country in enumerate(countries):
            if country in target_dict:
                target_tensor[j, 0] = target_dict[country]
        
        sequences.append(graphs)
        sequence_targets.append(target_tensor)
    
    return sequences, sequence_targets

def baseline_comparison(data: pd.DataFrame, targets: Dict[int, Dict[str, float]], 
                       countries: List[str]) -> Dict[str, float]:
    test_years = sorted([y for y in targets.keys() if y >= 2018])
    
    if len(test_years) < 2:
        return {'persistence': float('inf'), 'historical_avg': float('inf')}
    
    all_vulnerabilities = [v for year_targets in targets.values() for v in year_targets.values()]
    hist_avg = np.mean(all_vulnerabilities)
    
    persistence_errors, hist_avg_errors = [], []
    
    for year in test_years[1:]:
        if year in targets and year-1 in targets:
            current = np.array([targets[year].get(c, 0) for c in countries])
            previous = np.array([targets[year-1].get(c, 0) for c in countries])
            hist_avg_pred = np.full_like(current, hist_avg)
            
            persistence_errors.append(mean_absolute_error(current, previous))
            hist_avg_errors.append(mean_absolute_error(current, hist_avg_pred))
    
    return {
        'persistence': np.mean(persistence_errors) if persistence_errors else float('inf'),
        'historical_avg': np.mean(hist_avg_errors) if hist_avg_errors else float('inf')
    }

def run_analysis() -> Optional[ValidationResults]:
    logger.info("Starting wheat trade vulnerability analysis")
    
    config = ModelConfig()
    data_processor = DataProcessor()
    graph_builder = GraphBuilder(config)
    trainer = ModelTrainer(config)
    
    try:
        data = data_processor.load_wheat_data()
        consumption_baselines = data_processor.estimate_consumption_baselines(data)
        targets = data_processor.calculate_vulnerability_targets(data, consumption_baselines)
        countries = list(consumption_baselines.keys())
        
        logger.info(f"Processing {len(countries)} countries across {len(targets)} years")
        
        baseline_results = baseline_comparison(data, targets, countries)
        
        all_years = sorted(data['year'].unique())
        train_years = [y for y in all_years if y <= 2015]
        test_years = [y for y in all_years if y >= 2016]
        
        train_seq, train_tgt = prepare_sequences(data, targets, countries, train_years, config.sequence_length, graph_builder)
        val_seq, val_tgt = prepare_sequences(data, targets, countries, train_years[-3:], config.sequence_length, graph_builder)
        test_seq, test_tgt = prepare_sequences(data, targets, countries, test_years, config.sequence_length, graph_builder)
        
        if not train_seq:
            logger.error("No training sequences generated")
            return None
        
        logger.info(f"Generated {len(train_seq)} training sequences")
        
        train_data = list(zip(train_seq, train_tgt))
        val_data = list(zip(val_seq, val_tgt)) if val_seq else None
        test_data = list(zip(test_seq, test_tgt)) if test_seq else []
        
        model = trainer.train_model(train_data, val_data)
        
        if test_data:
            test_mae, predictions, actuals = trainer.evaluate_model(model, test_data)
            
            ss_res = np.sum((actuals - predictions) ** 2)
            ss_tot = np.sum((actuals - np.mean(actuals)) ** 2)
            r2_score = 1 - (ss_res / (ss_tot + 1e-8))
        else:
            test_mae, r2_score = float('inf'), 0.0
            predictions = np.array([])
        
        country_predictions = {}
        if len(predictions) > 0:
            country_predictions = {countries[i % len(countries)]: float(predictions[i]) 
                                 for i in range(min(len(countries), len(predictions)))}
        
        logger.info("=" * 60)
        logger.info("WHEAT VULNERABILITY ANALYSIS RESULTS")
        logger.info("=" * 60)
        logger.info(f"Test MAE: {test_mae:.4f}")
        logger.info(f"Test RÂ²: {r2_score:.4f}")
        logger.info(f"Baseline persistence MAE: {baseline_results['persistence']:.4f}")
        logger.info(f"Baseline historical avg MAE: {baseline_results['historical_avg']:.4f}")
        
        if test_mae != float('inf') and baseline_results['persistence'] != float('inf'):
            improvement = (baseline_results['persistence'] - test_mae) / baseline_results['persistence'] * 100
            logger.info(f"Improvement vs persistence: {improvement:.1f}%")
        
        results = ValidationResults(
            cv_mae=0.0,
            test_mae=test_mae,
            baseline_persistence=baseline_results['persistence'],
            baseline_historical=baseline_results['historical_avg'],
            r2_score=r2_score,
            predictions=country_predictions
        )
        
        return results
        
    except Exception as e:
        logger.error(f"Analysis failed: {e}")
        raise

if __name__ == "__main__":
    results = run_analysis()
    if results:
        logger.info("Analysis completed successfully")
    else:
        logger.error("Analysis failed to complete")
